{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2SeqAnomalyDetection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fastforwardlabs/anomaly_detection/blob/master/seq2seq/Seq2SeqAnomalyDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV-m_dvRcGnQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "e1015a7c-5cda-4309-a927-5c974d23e615"
      },
      "source": [
        "!wget http://www.timeseriesclassification.com/Downloads/ECG5000.zip\n",
        "\n",
        "!mkdir data\n",
        "!mkdir models\n",
        "!mv ECG5000.zip data/ECG5000.zip\n",
        "!unzip data/ECG5000.zip -d data/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data/ECG5000.zip\n",
            "  inflating: data/ECG5000.txt        \n",
            "  inflating: data/ECG5000_TEST.arff  \n",
            "  inflating: data/ECG5000_TEST.ts    \n",
            "  inflating: data/ECG5000_TEST.txt   \n",
            "  inflating: data/ECG5000_TRAIN.arff  \n",
            "  inflating: data/ECG5000_TRAIN.ts   \n",
            "  inflating: data/ECG5000_TRAIN.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFeoCHf9cc19",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db7ed10e-81b4-4340-ad34-debb6ec2b2a5"
      },
      "source": [
        "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "def model(n_features, encoder_dim = [20], decoder_dim = [20], dropout=0., learning_rate=.001, \n",
        "          loss='mean_squared_error', output_activation='sigmoid'):\n",
        "    \"\"\" Build seq2seq model.\n",
        "    \n",
        "    Arguments:\n",
        "        - n_features (int): number of features in the data\n",
        "        - encoder_dim (list): list with number of units per encoder layer\n",
        "        - decoder_dim (list): list with number of units per decoder layer\n",
        "        - dropout (float): dropout for LSTM units\n",
        "        - learning_rate (float): learning rate used during training\n",
        "        - loss (str): loss function used\n",
        "        - output_activation (str): activation type for the dense output layer in the decoder\n",
        "    \"\"\"\n",
        "    \n",
        "    enc_dim = len(encoder_dim)\n",
        "    dec_dim = len(decoder_dim)\n",
        "    \n",
        "    # seq2seq = encoder + decoder\n",
        "    # encoder\n",
        "    encoder_hidden = encoder_inputs = Input(shape=(None, n_features), name='encoder_input')\n",
        "    \n",
        "    # add encoder hidden layers\n",
        "    encoder_lstm = []\n",
        "    for i in range(enc_dim-1):\n",
        "        encoder_lstm.append(Bidirectional(LSTM(encoder_dim[i], dropout=dropout, \n",
        "                                               return_sequences=True,name='encoder_lstm_' + str(i))))\n",
        "        encoder_hidden = encoder_lstm[i](encoder_hidden)\n",
        "    \n",
        "    encoder_lstm.append(Bidirectional(LSTM(encoder_dim[-1], dropout=dropout, return_state=True, \n",
        "                                           name='encoder_lstm_' + str(enc_dim-1))))\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm[-1](encoder_hidden)\n",
        "    \n",
        "    # only need to keep encoder states\n",
        "    state_h = Concatenate()([forward_h, backward_h])\n",
        "    state_c = Concatenate()([forward_c, backward_c])\n",
        "    encoder_states = [state_h, state_c]\n",
        "    \n",
        "    # decoder\n",
        "    decoder_hidden = decoder_inputs = Input(shape=(None, n_features), name='decoder_input')\n",
        "    \n",
        "    # add decoder hidden layers\n",
        "    # check if dimensions are correct\n",
        "    dim_check = [(idx,dim) for idx,dim in enumerate(decoder_dim) if dim!=encoder_dim[-1]*2]\n",
        "    if len(dim_check)>0:\n",
        "        raise ValueError('\\nDecoder (layer,units) {0} is not compatible with encoder hidden ' \\\n",
        "                         'states. Units should be equal to {1}'.format(dim_check,encoder_dim[-1]*2))\n",
        "    \n",
        "    # initialise decoder states with encoder states\n",
        "    decoder_lstm = []\n",
        "    for i in range(dec_dim):\n",
        "        decoder_lstm.append(LSTM(decoder_dim[i], dropout=dropout, return_sequences=True,\n",
        "                                 return_state=True, name='decoder_lstm_' + str(i)))\n",
        "        decoder_hidden, _, _ = decoder_lstm[i](decoder_hidden, initial_state=encoder_states)\n",
        "    \n",
        "    # add linear layer on top of LSTM\n",
        "    decoder_dense = Dense(n_features, activation=output_activation, name='dense_output')\n",
        "    decoder_outputs = decoder_dense(decoder_hidden)\n",
        "    \n",
        "    # define seq2seq model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    optimizer = Adam(lr=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=loss)\n",
        "    \n",
        "    # define encoder model returning encoder states\n",
        "    encoder_model = Model(encoder_inputs, encoder_states * dec_dim)\n",
        "\n",
        "    # define decoder model\n",
        "    # need state inputs for each LSTM layer\n",
        "    decoder_states_inputs = []\n",
        "    for i in range(dec_dim):\n",
        "        decoder_state_input_h = Input(shape=(decoder_dim[i],), name='decoder_state_input_h_' + str(i))\n",
        "        decoder_state_input_c = Input(shape=(decoder_dim[i],), name='decoder_state_input_c_' + str(i))\n",
        "        decoder_states_inputs.append([decoder_state_input_h, decoder_state_input_c])\n",
        "    decoder_states_inputs = [state for states in decoder_states_inputs for state in states]\n",
        "    \n",
        "    decoder_inference = decoder_inputs\n",
        "    decoder_states = []\n",
        "    for i in range(dec_dim):\n",
        "        decoder_inference, state_h, state_c = decoder_lstm[i](decoder_inference, \n",
        "                                                              initial_state=decoder_states_inputs[2*i:2*i+2])\n",
        "        decoder_states.append([state_h,state_c])\n",
        "    decoder_states = [state for states in decoder_states for state in states]\n",
        "    \n",
        "    decoder_outputs = decoder_dense(decoder_inference)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
        "                          [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return model, encoder_model, decoder_model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkU9HAwpildC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwBw8mdZczLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "from scipy.io import arff\n",
        "\n",
        "# from model import model\n",
        "\n",
        "np.random.seed(2018)\n",
        "np.random.RandomState(2018)\n",
        "random.seed(2018)\n",
        "\n",
        "# default args\n",
        "DATASET = './data/ECG5000_TEST.arff'\n",
        "SAVE_PATH = './models/'\n",
        "MODEL_NAME = 'seq2seq'\n",
        "DATA_RANGE = [0,2627]\n",
        "\n",
        "# data preprocessing\n",
        "STANDARDIZED = False\n",
        "MINMAX = False\n",
        "CLIP = [99999]\n",
        "\n",
        "# architecture\n",
        "TIMESTEPS = 140 # length of 1 ECG\n",
        "ENCODER_DIM = [20]\n",
        "DECODER_DIM = [40]\n",
        "OUTPUT_ACTIVATION = 'sigmoid'\n",
        "\n",
        "# training\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = .005\n",
        "LOSS = 'mean_squared_error'\n",
        "DROPOUT = 0.\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SAVE = True\n",
        "PRINT_PROGRESS = True\n",
        "CONTINUE_TRAINING = False\n",
        "LOAD_PATH = SAVE_PATH\n",
        "\n",
        "def train(model,X,args):\n",
        "    \"\"\" Train seq2seq-LSTM model. \"\"\"\n",
        "    \n",
        "    # clip data per feature\n",
        "    for col,clip in enumerate(args.clip):\n",
        "        X[:,:,col] = np.clip(X[:,:,col],-clip,clip)\n",
        "    \n",
        "    # apply scaling and save data preprocessing method\n",
        "    axis = (0,1) # scaling per feature over all observations\n",
        "    if args.standardized:\n",
        "        print('\\nStandardizing data')\n",
        "        mu, sigma = np.mean(X,axis=axis), np.std(X,axis=axis)\n",
        "        X = (X - mu) / (sigma + 1e-10)\n",
        "        \n",
        "        with open(args.save_path + 'preprocess_' + args.model_name + '.pickle', 'wb') as f:\n",
        "            pickle.dump(['standardized',args.clip,axis,mu,sigma], f)\n",
        "    \n",
        "    if args.minmax:\n",
        "        print('\\nMinmax scaling of data')\n",
        "        xmin, xmax = X.min(axis=axis), X.max(axis=axis)\n",
        "        min, max = 0, 1\n",
        "        X = ((X - xmin) / (xmax - xmin)) * (max - min) + min\n",
        "        \n",
        "        with open(args.save_path + 'preprocess_' + args.model_name + '.pickle', 'wb') as f:\n",
        "            pickle.dump(['minmax',args.clip,axis,xmin,xmax,min,max], f)\n",
        "            \n",
        "    # define inputs\n",
        "    encoder_input_data = X\n",
        "    decoder_input_data = X\n",
        "    decoder_target_data = np.roll(X, -1, axis=1) # offset decoder_input_data by 1 across time axis\n",
        "\n",
        "    # set training arguments\n",
        "    if args.print_progress:\n",
        "        verbose = 1\n",
        "    else:\n",
        "        verbose = 0\n",
        "\n",
        "    kwargs = {}\n",
        "    kwargs['epochs'] = args.epochs\n",
        "    kwargs['batch_size'] = args.batch_size\n",
        "    kwargs['shuffle'] = True\n",
        "    kwargs['validation_split'] = args.validation_split\n",
        "    kwargs['verbose'] = verbose\n",
        "\n",
        "    if args.save: # create callback\n",
        "        print('\\nSave stuff')\n",
        "        checkpointer = ModelCheckpoint(filepath=args.save_path + args.model_name + '_weights.h5',verbose=0,\n",
        "                                       save_best_only=True,save_weights_only=True)\n",
        "        kwargs['callbacks'] = [checkpointer]\n",
        "        \n",
        "        # save model architecture\n",
        "        with open(args.save_path + args.model_name + '.pickle', 'wb') as f:\n",
        "            pickle.dump([X.shape[1],X.shape[2],args.encoder_dim,\n",
        "                         args.decoder_dim,args.output_activation],f)\n",
        "    \n",
        "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data, **kwargs)\n",
        "\n",
        "def run(args):\n",
        "    \"\"\" Load data, generate training batch, initiate and train model. \"\"\"\n",
        "    \n",
        "    print('\\nLoad dataset')\n",
        "    data = arff.loadarff(args.dataset)\n",
        "    data = pd.DataFrame(data[0])\n",
        "    data.drop(columns='target',inplace=True)\n",
        "    \n",
        "    print('\\nGenerate training batch')\n",
        "    args.n_features = 1 # only 1 feature in the ECG dataset\n",
        "    X = data.values[args.data_range[0]:args.data_range[1],:]\n",
        "    X = np.reshape(X, (X.shape[0],X.shape[1],args.n_features))\n",
        "    \n",
        "    print('\\nInitiate outlier detector model')\n",
        "    s2s, enc, dec = model(args.n_features,encoder_dim=args.encoder_dim,decoder_dim=args.decoder_dim,\n",
        "                          dropout=args.dropout,learning_rate=args.learning_rate,loss=args.loss,\n",
        "                          output_activation=args.output_activation)\n",
        "    \n",
        "    if args.continue_training:\n",
        "        print('\\nLoad pre-trained model')\n",
        "        s2s.load_weights(args.load_path + args.model_name + '_weights.h5') # load pretrained model weights\n",
        "    \n",
        "    if args.print_progress:\n",
        "        s2s.summary()\n",
        "    \n",
        "    print('\\nTrain outlier detector')\n",
        "    train(s2s,X,args)\n",
        "    \n",
        "# if __name__ == '__main__':\n",
        "import sys; sys.argv=['']; del sys\n",
        "    \n",
        "parser = argparse.ArgumentParser(description=\"Train seq2seq-LSTM outlier detector.\")\n",
        "parser.add_argument('--dataset',type=str,choices=DATASET,default=DATASET)\n",
        "parser.add_argument('--data_range',type=int,nargs=2,default=DATA_RANGE)\n",
        "parser.add_argument('--timesteps',type=int,default=TIMESTEPS)\n",
        "parser.add_argument('--encoder_dim',type=int,nargs='+',default=ENCODER_DIM)\n",
        "parser.add_argument('--decoder_dim',type=int,nargs='+',default=DECODER_DIM)\n",
        "parser.add_argument('--output_activation',type=str,default=OUTPUT_ACTIVATION)\n",
        "parser.add_argument('--dropout',type=float,default=DROPOUT)\n",
        "parser.add_argument('--learning_rate',type=float,default=LEARNING_RATE)\n",
        "parser.add_argument('--loss',type=str,default=LOSS)\n",
        "parser.add_argument('--validation_split',type=float,default=VALIDATION_SPLIT)\n",
        "parser.add_argument('--epochs',type=int,default=EPOCHS)\n",
        "parser.add_argument('--batch_size',type=int,default=BATCH_SIZE)\n",
        "parser.add_argument('--clip',type=float,nargs='+',default=CLIP)\n",
        "parser.add_argument('--standardized', default=STANDARDIZED, action='store_true')\n",
        "parser.add_argument('--minmax', default=MINMAX, action='store_true')\n",
        "parser.add_argument('--print_progress', default=PRINT_PROGRESS, action='store_true')\n",
        "parser.add_argument('--save', default=SAVE, action='store_true')\n",
        "parser.add_argument('--save_path',type=str,default=SAVE_PATH)\n",
        "parser.add_argument('--load_path',type=str,default=LOAD_PATH)\n",
        "parser.add_argument('--model_name',type=str,default=MODEL_NAME)\n",
        "parser.add_argument('--continue_training', default=CONTINUE_TRAINING, action='store_true')\n",
        "args = parser.parse_args()\n",
        "\n",
        "run(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi-rIz1k-m24",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fff80010-d534-4f18-f0fb-54ad7abf93c8"
      },
      "source": [
        "for col,clip in enumerate(args.clip):\n",
        "  print(col,clip)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 99999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9SJBdITsToG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s2s, enc, dec = model(args.n_features,encoder_dim=args.encoder_dim,decoder_dim=args.decoder_dim,\n",
        "                          dropout=args.dropout,learning_rate=args.learning_rate,loss=args.loss,\n",
        "                          output_activation=args.output_activation)\n",
        "s2s.load_weights(SAVE_PATH + MODEL_NAME + '_weights.h5')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0Xp1fc_uXCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "e5fd3a60-b66a-4736-a42b-3a42f8d369d3"
      },
      "source": [
        "data = arff.loadarff(args.dataset)\n",
        "data = pd.DataFrame(data[0])\n",
        "data.drop(columns='target',inplace=True)\n",
        "\n",
        "print('\\nGenerate training batch')\n",
        "args.n_features = 1 # only 1 feature in the ECG dataset\n",
        "X = data.values[args.data_range[0]:args.data_range[1],:]\n",
        "X = np.reshape(X, (X.shape[0],X.shape[1],args.n_features))\n",
        "s2s.predict(X[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Generate training batch\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-07d0987766b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0ms2s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[ 3.6908442e+00],\n       [ 7.1141435e-01],\n       [-2.1140915e+00],\n       [-4.1410068e+00],\n       [-4.5744716e+00],\n       [-3.4319085e+00],\n       [-1.9507914e+00],\n       [-1.1070667e+00],..."
          ]
        }
      ]
    }
  ]
}